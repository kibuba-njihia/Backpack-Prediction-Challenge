{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90274,"databundleVersionId":10995111,"sourceType":"competition"},{"sourceId":10631375,"sourceType":"datasetVersion","datasetId":6582377}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing Modules:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport warnings \nwarnings.filterwarnings('ignore')\n# Disable LightGBM warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport logging\nlogging.getLogger('lightgbm').setLevel(logging.INFO)\nlogging.getLogger('lightgbm').setLevel(logging.ERROR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Predicting Backpack price:","metadata":{}},{"cell_type":"markdown","source":"# Importing Data:","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(r\"/kaggle/input/playground-series-s5e2/train.csv\")\ntest_data = pd.read_csv(r\"/kaggle/input/playground-series-s5e2/test.csv\")\ndata = pd.read_csv(r\"/kaggle/input/playground-series-s5e2/training_extra.csv\")\nsample_submission = pd.read_csv(r\"/kaggle/input/playground-series-s5e2/sample_submission.csv\")\n\nprint(\"train_data shape :\",train_data.shape)\nprint(\"test_data shape :\",test_data.shape)\nprint(\"data shape :\",data.shape)\nprint(\"sample_submission shape :\",sample_submission.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Identify categorical columns\ncat_cols = train_data.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\n# Define subplot grid (3 rows, auto columns)\nnum_plots = len(cat_cols)\nrows = 3\ncols = (num_plots // rows) + (num_plots % rows > 0)  # Adjust columns dynamically\n\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\naxes = axes.flatten()  # Flatten for easy indexing\n\n# Plot each categorical column\nfor i, col in enumerate(cat_cols):\n    value_counts = train_data[col].value_counts()\n    axes[i].pie(value_counts, labels=value_counts.index, autopct=\"%1.1f%%\", startangle=140)\n    axes[i].set_title(f\"Distribution of {col}\")\n\n# Hide unused subplots (if any)\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.isna().sum().sort_values(ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values\nmissing_values = train_data.isnull().mean() * 100\n\n# Plot\nmissing_values.plot(kind='bar', figsize=(10, 6), color='skyblue')\nplt.title('Percentage of Missing Values by Feature')\nplt.ylabel('Percentage')\nplt.xlabel('Features')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_data = train_data.dropna()\ntrain_data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values\nmissing_values = data.isnull().mean() * 100\n\n# Plot\nmissing_values.plot(kind='bar', figsize=(10, 6), color='skyblue')\nplt.title('Percentage of Missing Values by Feature')\nplt.ylabel('Percentage')\nplt.xlabel('Features')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = data.dropna()\ndata.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of the data:\n#data.hist(figsize=(10,5),color = 'skyblue', edgecolor='black')\n#plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of the data:\n#train_data.drop(['id'],axis=1).hist(figsize=(10,5),color = 'skyblue', edgecolor='black')\n#plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.isna().sum().sort_values(ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values\nmissing_values = test_data.isnull().mean() * 100\n\n# Plot\nmissing_values.plot(kind='bar', figsize=(10, 6), color='skyblue')\nplt.title('Percentage of Missing Values by Feature')\nplt.ylabel('Percentage')\nplt.xlabel('Features')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Merging original and train data:","metadata":{}},{"cell_type":"code","source":"#train_data = train_data.drop(\"id\", axis=1)\n#test_data = test_data.drop(\"id\", axis=1)\ntrain_data = pd.concat([train_data, data], ignore_index=True)\ntrain_data = train_data.drop(\"id\", axis=1)\ntrain_data = train_data.drop_duplicates()\nprint(\"shape of the data :\",train_data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_data = train_data.fillna('missing')\n#test_data = test_data.fillna('missing')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_data = train_data.drop('id', axis = 1)\nnum_cols = list(train_data.select_dtypes(exclude=['object']).columns.difference(['Price']))\ncat_cols = list(train_data.select_dtypes(include=['object']).columns)\n\nnum_cols_test = list(test_data.select_dtypes(exclude=['object']).columns.difference(['id']))\ncat_cols_test = list(test_data.select_dtypes(include=['object']).columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill missing values\ntrain_data[train_data.select_dtypes(include=['number']).columns] = train_data.select_dtypes(include=['number']).apply(lambda x: x.fillna(x.median()))\ntrain_data[train_data.select_dtypes(include=['object']).columns] = train_data.select_dtypes(include=['object']).apply(lambda x: x.fillna(\"missing\").astype('category'))\n\n# Fill missing values\ntest_data[test_data.select_dtypes(include=['number']).columns] = test_data.select_dtypes(include=['number']).apply(lambda x: x.fillna(x.median()))\ntest_data[test_data.select_dtypes(include=['object']).columns] = test_data.select_dtypes(include=['object']).apply(lambda x: x.fillna(\"missing\").astype('category'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using ANOVA (F-statistic)\nANOVA tests whether the means of the target variable differ significantly across different categories. Higher F-values indicate stronger correlation.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import f_oneway\n\ndef anova_correlation(train_data, cat_cols, target_col):\n    results = {}\n    for col in cat_cols:\n        groups = [train_data[target_col][train_data[col] == category] for category in train_data[col].unique()]\n        f_stat, p_value = f_oneway(*groups)\n        results[col] = f_stat  # Higher F-statistic means stronger relation\n    return results\n\n# Identify categorical columns\n#categorical_cols = ['cat_col1', 'cat_col2']  # Replace with your categorical column names\nanova_results = anova_correlation(train_data, cat_cols, 'Price')\n\n# Display sorted results\nsorted(anova_results.items(), key=lambda x: x[1], reverse=True)\n\n#Interpretation: Higher F-values mean stronger correlation between the categorical column and the numerical target.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using Mean Encoding with Pearson Correlation\nAnother approach is to replace each category with its mean target value and then compute Pearson correlation.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import pearsonr\n\ndef mean_encoding_correlation(train_data, cat_cols, target_col):\n    correlations = {}\n    for col in cat_cols:\n        mean_encoded = train_data.groupby(col)[target_col].transform('mean')\n        correlation, _ = pearsonr(mean_encoded, train_data[target_col])\n        correlations[col] = correlation\n    return correlations\n\nmean_corr_results = mean_encoding_correlation(train_data, cat_cols, 'Price')\n\n# Display sorted results\nsorted(mean_corr_results.items(), key=lambda x: abs(x[1]), reverse=True)\n\n#Interpretation:\n#1. A high absolute correlation value (close to 1 or -1) indicates a strong relationship.\n#2. Values closer to 0 suggest little or no correlation.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encoding data:","metadata":{}},{"cell_type":"code","source":"# Convert all categorical columns to string type\n#train_data[cat_cols] = train_data[cat_cols].astype(str)\n#test_data[cat_cols] = test_data[cat_cols].astype(str)\n# Initialize LabelEncoder\nlabel_encoders = {col: LabelEncoder() for col in cat_cols}\n\n# Apply LabelEncoder to each categorical column\nfor col in cat_cols:\n    train_data[col] = label_encoders[col].fit_transform(train_data[col])\n    test_data[col] = label_encoders[col].transform(test_data[col])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Scaling data:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_data[num_cols] = scaler.fit_transform(train_data[num_cols])\ntest_data[num_cols_test] = scaler.transform(test_data[num_cols_test])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the figure and axes\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\n\n# Boxplot\nsns.boxplot(x=train_data[\"Price\"], ax=axes[0], color='lightblue')\naxes[0].set_title(\"Boxplot of Price\")\n\n# Histogram\nsns.histplot(train_data[\"Price\"], bins=30, kde=False, ax=axes[1], color='lightgreen')\naxes[1].set_title(\"Histogram of Price\")\n\n# Distribution plot (KDE + Histogram)\nsns.histplot(train_data[\"Price\"], bins=30, kde=True, ax=axes[2], color='salmon')\naxes[2].set_title(\"Distribution Plot of Price\")\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the correlation matrix\ncorrelation_matrix = train_data.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(10, 5))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\nplt.title('Feature Correlation Heatmap')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data['Compartments'] = train_data['Compartments'].astype('object')\ntest_data['Compartments'] = test_data['Compartments'].astype('object')\ntrain_data['Weight Capacity (kg)'] = train_data['Weight Capacity (kg)'].astype('object')\ntest_data['Weight Capacity (kg)'] = test_data['Weight Capacity (kg)'].astype('object')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting Data:","metadata":{}},{"cell_type":"code","source":"X = train_data.drop(['Price'], axis=1)\ny = train_data['Price']\ntest = test_data.drop(['id'],axis=1)\n\n# Split datainto training set and test set\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoostRegressor:","metadata":{}},{"cell_type":"code","source":"parameters ={'iterations': 900, 'depth': 5, 'learning_rate': 0.06687502193263943, 'l2_leaf_reg': 0.009012725660552562, 'border_count': 201, 'random_strength': 4.410487182794549, 'bagging_temperature': 6.804257115462372}\n#value: 38.64661057765384.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n# Convert categorical columns to string type\nfor col in X.columns:\n    X[col] = X[col].astype(str)\n    test[col] = test[col].astype(str)\n\n# Identify categorical features\ncat_features = X.columns.tolist()\n\n# Initialize K-Fold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-Fold CV\nrmse_scores = []\npreds = []\n\n# Perform K-Fold CV\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Convert to CatBoost Pool\n    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n    val_pool = Pool(X_val, y_val, cat_features=cat_features)\n    \n    # Initialize CatBoost Regressor\n    model = CatBoostRegressor(**parameters,\n        loss_function=\"RMSE\",\n        cat_features=cat_features,\n        verbose=0\n    )\n\n    # Train model\n    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, use_best_model=True)\n\n    # Predict & compute RMSE\n    y_pred = model.predict(X_val)\n    pred = model.predict(test)\n    preds.append(pred)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    rmse_scores.append(rmse)\n    print(f\"Fold RMSE: {rmse:.4f}\")\n\n# Print Mean RMSE across folds\nmean_rmse = np.mean(rmse_scores)\nprint(f\"\\nMean RMSE across folds: {mean_rmse:.4f}\")\n\ncat_preds = np.mean(preds, axis=0)\nsubmission_cat = pd.DataFrame({'id': test_data.id, 'Price': cat_preds})\nprint(submission_cat.head())\nsubmission_cat.to_csv('submission_cat.csv', index=False)\n\nsubmission_cat['Price'].hist()\n#Mean RMSE across folds: 38.6339","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Initialize K-Fold cross-validator\nkf = KFold(n_splits=5, shuffle=True)\n\n# Initialize lists to store results\nrmse_scores = []\npreds = []\n\n# Initialize the model\nmodel = CatBoostRegressor(**parameters,verbose=0,loss_function='RMSE',eval_metric= 'RMSE',random_state=42)\n\n# K-Fold Cross-Validation\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Fit the model\n    model.fit(X_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    pred = model.predict(test)\n    preds.append(pred)\n\n    # Calculate RMSE\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    rmse_scores.append(rmse)\n\n# Calculate the average RMSE across all folds\navg_rmse = np.mean(rmse_scores)\n\n# Print RMSE for each fold and the average RMSE\nprint(\"RMSE scores for each fold:\", rmse_scores)\nprint(\"Average RMSE:\", avg_rmse)\n\ncat_preds = np.mean(preds, axis=0)\nsubmission_cat = pd.DataFrame({'id': test_data.id, 'Price': cat_preds})\nprint(submission_cat.head())\nsubmission_cat.to_csv('submission_cat1.csv', index=False)\n\nsubmission_cat['Price'].hist()\n#Average RMSE: 38.7940188825754","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBRegressor:","metadata":{}},{"cell_type":"code","source":"xgb_parameters = {'n_estimators': 3649, 'learning_rate': 0.03618354865553652, 'max_depth': 3, 'min_child_weight': 4.711257151527733, 'subsample': 0.7574455337411672, 'colsample_bytree': 0.9119856086671668, 'lambda': 0.000937201633995687, 'alpha': 3.060383912120667}\n#Best RMSE: 38.9996526675085\nparameters = {'n_estimators': 1980, 'learning_rate': 0.019016829187088413, 'max_depth': 4, 'min_child_weight': 1.2576362814382727, 'subsample': 0.8551673209374542, 'colsample_bytree': 0.9666608128642872, 'lambda': 3.353364908451519e-05, 'alpha': 1.0945466337127865}\n#value: 38.793353567386134.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n# Initialize K-Fold cross-validator\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Initialize lists to store results\nrmse_scores = []\npreds = []\n# Initialize the model\nmodel = XGBRegressor(**parameters,loss_function='RMSE',random_state=42)\n\n# K-Fold Cross-Validation\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Fit the model\n    model.fit(X_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    pred = model.predict(test)\n    preds.append(pred)\n\n    # Calculate RMSE\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    rmse_scores.append(rmse)\n\n# Calculate the average RMSE across all folds\navg_rmse = np.mean(rmse_scores)\n\n# Print RMSE for each fold and the average RMSE\nprint(\"RMSE scores for each fold:\", rmse_scores)\nprint(\"Average RMSE:\", avg_rmse)\n\nxgb_preds = np.mean(preds, axis=0)\nsubmission_xgb = pd.DataFrame({'id': test_data.id, 'Price': xgb_preds})\nprint(submission_xgb.head())\nsubmission_xgb.to_csv('submission_xgb.csv', index=False)\n\nsubmission_xgb['Price'].hist()","metadata":{}},{"cell_type":"markdown","source":"# LGBMRegressor:","metadata":{}},{"cell_type":"code","source":"lgb_parameters = {'n_estimators': 1267, 'learning_rate': 0.010657512662509413, 'max_depth': 3, 'num_leaves': 42, 'min_child_samples': 48, 'subsample': 0.5643136402119217, 'colsample_bytree': 0.8624225587120392, 'reg_alpha': 0.011441404961226555, 'reg_lambda': 0.0712926757533912}\n#Best RMSE: 39.01458863233762\nlgbparameters = {'n_estimators': 545, 'learning_rate': 0.022486051953875208, 'max_depth': 3, 'num_leaves': 115, 'min_child_samples': 76, 'subsample': 0.995811529438896, 'colsample_bytree': 0.7158057062872807, 'reg_alpha': 0.004169813199994837, 'reg_lambda': 0.4756560131839915}\n#Best RMSE: 39.01198791552696","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Initialize K-Fold cross-validator\nkf = KFold(n_splits=5, shuffle=True)\n\n# Initialize lists to store results\nrmse_scores = []\npreds = []\n\n# Initialize the model\nmodel = LGBMRegressor(**lgb_parameters,verbosity=-1,loss_function='RMSE',eval_metric= 'RMSE',random_state=42,boosting_type=\"gbdt\")\n\n# K-Fold Cross-Validation\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Fit the model\n    model.fit(X_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    pred = model.predict(test)\n    preds.append(pred)\n\n    # Calculate RMSE\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    rmse_scores.append(rmse)\n\n# Calculate the average RMSE across all folds\navg_rmse = np.mean(rmse_scores)\n\n# Print RMSE for each fold and the average RMSE\nprint(\"RMSE scores for each fold:\", rmse_scores)\nprint(\"Average RMSE:\", avg_rmse)\n\nlgb_preds =  np.mean(preds, axis=0)\nsubmission_lgb = pd.DataFrame({'id': test_data.id, 'Price': lgb_preds})\nprint(submission_lgb.head())\nsubmission_lgb.to_csv('submission_lgb.csv', index=False)\n\nsubmission_lgb['Price'].hist()","metadata":{}},{"cell_type":"markdown","source":"preds = cat_preds*0.22 + lgb_preds*0.28 + xgb_preds*0.5\nsubmission = pd.DataFrame({'id': test_data.id, 'Price': preds})\nprint(submission.head())\nsubmission.to_csv('submission_blend.csv', index=False)\n\nsubmission['Price'].hist()","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}